# -*- coding: utf-8 -*-
"""TrainDPO

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ozeRmghmeUOEF1-kuCC2vYpbC4ywEoG
"""

from datasets import load_dataset
from trl import DPOConfig, DPOTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer
"""Load/format dataset"""

jdgfct = "Factuality"

train_dataset = load_dataset("chardizard/dpo-mix5-Llama3-Factuality", split="train")

minchosen = 9
mindelta = 6
train_dataset_filtered = train_dataset.filter(lambda x: x['chosen_score'] >= minchosen and x['chosen_score']-x['rejected_score'] >= mindelta)
del train_dataset
print("Filtered dataset length: "+str(len(train_dataset_filtered)))

# train_dataset_filtered.push_to_hub(f"chardizard/dpo-mix5-Llama3-{jdgfct}-MinChosen{minchosen}-MinDelta{mindelta}")

train_dataset_filtered = train_dataset_filtered.remove_columns(['system', 'chosen_score', 'rejected_score']).rename_column('question', 'prompt')

"""Model"""

model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
ref_model = AutoModelForCausalLM.from_pretrained(model_name)

training_args = DPOConfig(output_dir=f"Qwen2.5-7b-DPO-Factuality-MinChosen{minchosen}-MinDelta{mindelta}",
num_train_epochs=1,
bf16=True,
per_device_train_batch_size=1,
gradient_accumulation_steps=4,
push_to_hub=True)

trainer = DPOTrainer(model=model, ref_model=ref_model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset_filtered)

trainer.train()

trainer.save_model("/vast/rhz2020/models")
